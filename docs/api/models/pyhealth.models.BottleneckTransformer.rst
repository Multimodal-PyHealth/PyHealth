BottleneckTransformer
---------------------

.. autoclass:: pyhealth.models.BottleneckTransformer
   :members:
   :undoc-members:
   :show-inheritance:

**Overview**

``BottleneckTransformer`` is a multimodal architecture based on *Attention Bottlenecks for Multimodal Fusion* (NeurIPS 2021). It uses shared bottleneck tokens to fuse representations across an arbitrary number of modalities—restricting cross-modal attention to improve compute efficiency and prevent noise from dominating specific modalities.

**Input / Output**

- **Input:** ``dict[str, Tensor]``
  — kwargs dictionary mapping from PyHealth's input_schema fields to tensor data (sequences, vectors, or multimodal tokens).
- **Output:** ``dict`` with keys:

  - ``"y_prob"`` — ``(B, num_classes)`` predicted probabilities
  - ``"y_true"`` — ``(B, num_classes)`` true labels (if provided)
  - ``"logit"``  — ``(B, num_classes)`` raw logits 
  - ``"loss"``   — scalar tensor (if true labels are provided)

**Key Features**

.. list-table::
   :header-rows: 1
   :widths: 20 80

   * - Feature
     - Description
   * - **Bottleneck Fusion**
     - Modalities interact solely through shared bottleneck tokens
   * - **Dynamic Modality Support**
     - Automatically adapts its encoder branches depending on ``dataset.input_schema`` length
   * - **Pre-fusion Encoding**
     - Intra-modal feature processing occurs independently up to ``fusion_startidx`` layer
   * - **Token Masking Support**
     - Safely processes ragged inputs dynamically generated by sequences with a ``mask``

**Example Usage**

.. code-block:: python

    from pyhealth.datasets import create_sample_dataset
    from pyhealth.models import BottleneckTransformer

    # Build multimodal dataset
    dataset = create_sample_dataset(
        samples=samples,
        input_schema={"conditions": "sequence", "procedures": "sequence"},
        output_schema={"label": "binary"}
    )

    # Initialize model
    model = BottleneckTransformer(
        dataset=dataset,
        embedding_dim=128,
        bottlenecks_n=4,      # Number of bottleneck tokens
        fusion_startidx=2,    # Layer index to begin cross-modal fusion
        num_layers=4,
        heads=4
    )

.. autoclass:: pyhealth.models.bottleneck_transformer.MultimodalBottleneckTransformerEncoder
   :members:
   :undoc-members:
   :show-inheritance:
